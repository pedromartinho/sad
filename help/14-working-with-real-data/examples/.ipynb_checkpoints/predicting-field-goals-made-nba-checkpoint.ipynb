{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting field goals made from NBA shot logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will state our imports, as usual. You will find that we are using the `preprocessing` module of `sklearn`, in addition to `pandas` and `numpy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy  as np\n",
    "\n",
    "% matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn import preprocessing as pp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Read data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by loading the data. For convenience, we will make all column names upper case, so we don't have to consider that throughout the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_data(path, with_preview=False):\n",
    "    \n",
    "    data = pd.read_csv(path)\n",
    "    data.columns = data.columns.str.upper()\n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    "data = read_data(\"../data/shot_logs.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Extracting features from the `MATCHUP` column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As in many real world problems, we have a column holding information about more than one feature.\n",
    "\n",
    "So we can use them throughout the example and the practice exercise we need to extract all the feature to separate columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_features(string):\n",
    "    \n",
    "    (date, string) = string.split('-')\n",
    "    \n",
    "    if '@' in string:\n",
    "        (away, home) = string.split('@')\n",
    "    if 'vs.' in string:\n",
    "        (away, home) = string.split('vs.')\n",
    "   \n",
    "    date = date.strip()\n",
    "    away = away.strip()\n",
    "    home = home.strip()\n",
    "    \n",
    "    return (date, away, home)\n",
    "    \n",
    "    \n",
    "def extract_date(string):\n",
    "    \n",
    "    features = extract_features(string)\n",
    "    \n",
    "    date = features[0]\n",
    "    date = pd.to_datetime(date)\n",
    "    date = date\n",
    "    \n",
    "    return date\n",
    "\n",
    "\n",
    "def extract_away_team(string):\n",
    "    \n",
    "    features = extract_features(string)\n",
    "    away = features[1]\n",
    "    \n",
    "    return away\n",
    "\n",
    "\n",
    "def extract_home_team(string):\n",
    "    \n",
    "    features = extract_features(string)\n",
    "    home = features[2]\n",
    "    \n",
    "    return home\n",
    "    \n",
    "    \n",
    "def expand_matchup_column(data, with_preview=False):\n",
    "    \n",
    "    data['MATCHUP_DATE']      = data['MATCHUP'].apply(extract_date)\n",
    "    data['MATCHUP_DATE']      = data['MATCHUP_DATE'].astype(np.int64) / int(1e6)\n",
    "    data['MATCHUP_AWAY_TEAM'] = data['MATCHUP'].apply(extract_away_team)\n",
    "    data['MATCHUP_HOME_TEAM'] = data['MATCHUP'].apply(extract_home_team)\n",
    "    \n",
    "    data = data.drop('MATCHUP', axis=1)\n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    "data = expand_matchup_column(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Dealing categorical features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see below, there are plenty of categorical features in our dataset.\n",
    "\n",
    "We will grab one of them, the `MATCHUP_AWAY_TEAM`, and perform different types of encoding on it, so you can get a sense of what they do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(data.head(n=3))\n",
    "data.columns.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Factorizing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the categorical features in our dataset is the `MATCHUP_AWAY_TEAM`.\n",
    "\n",
    "Let's label encoding it using factorization, and preview the results in a new dataframe with two columns: the old nominal values and the new, numeric ones.\n",
    "\n",
    "We will sample 5 rows from the resulting dataframe to interpret the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def factorize_column(data, colname):\n",
    "    \n",
    "    categorical_feature = data[colname]\n",
    " \n",
    "    categorical_feature_encoded = pd.factorize(categorical_feature)[0]\n",
    "    \n",
    "    return categorical_feature_encoded\n",
    "\n",
    "\n",
    "def compare_with_original_column(data, colname, encoded_column):\n",
    "\n",
    "    comparison = pd.DataFrame()\n",
    "    comparison['NOMINAL'] = data[colname]\n",
    "    comparison['ORDINAL'] = encoded_column\n",
    "    \n",
    "    comparison_sample = comparison.sample(n=5)\n",
    "    \n",
    "    return comparison_sample\n",
    "\n",
    "\n",
    "encoded_column = factorize_column(data, 'MATCHUP_AWAY_TEAM')\n",
    "compare_with_original_column(data, 'MATCHUP_AWAY_TEAM', encoded_column)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, all our nominal labels were now replaced by numeric values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Label encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will try to do the exact same thing, only using label encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_encode_column(data, colname):\n",
    "    \n",
    "    categorical_feature = data[colname]\n",
    "    \n",
    "    encoder = pp.LabelEncoder()\n",
    "    categorical_feature_encoded = encoder.fit_transform(categorical_feature)\n",
    "    \n",
    "    return categorical_feature_encoded\n",
    "\n",
    "\n",
    "encoded_column = label_encode_column(data, 'MATCHUP_AWAY_TEAM')\n",
    "compare_with_original_column(data, 'MATCHUP_AWAY_TEAM', encoded_column)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, we get similar results: our nominal categorical variable is transformed into a numerical one.\n",
    "\n",
    "As we know, this is a problem: we assume that teams have a natural order, meaning that Miami is great than Cleveland, for example. \n",
    "\n",
    "We also assume that the distances between cities are known, which they are not.\n",
    "\n",
    "Thus, this is a call for us to use dummy encoding, as we have seen in the lecture!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Dummy encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As stated above, the away team is not an ordinal feature: it has no natural order, nor known distances.\n",
    "\n",
    "Therefore we will use dummy encoding to binary encode each level, using 0 or 1 to indicate the absence or presence of some label for each sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dummy_encode_column(data, colname):\n",
    "    \n",
    "    categorical_feature = data[colname]\n",
    "    number_of_levels = len(categorical_feature.unique())\n",
    "    \n",
    "    dummy_encode_example = pd.get_dummies(categorical_feature, drop_first=True)\n",
    "    dummy_encode_number_of_cols = dummy_encode_example.shape[1]\n",
    "    \n",
    "    dummy_encode_sample = dummy_encode_example\n",
    "    print(\"There are %s total levels for the categorical variable %s.\" % (number_of_levels, colname))\n",
    "    print(\"There are %s (n-1) columns in the dummy encoded dataframe.\" % dummy_encode_number_of_cols)\n",
    "    \n",
    "    return dummy_encode_sample\n",
    "\n",
    "\n",
    "dummy_encode_column(data, 'MATCHUP_AWAY_TEAM').sample(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please note that we need to explicitly use `drop_first=True` to get n-1 (leave one out) columns in the encoded dataframe.\n",
    "\n",
    "Also, we are using `get_dummies` directly on a categorical feature containing strings; this is something we will not be able to do in our next example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4. One Hot Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to apply the one hot encoding similarly to what we did with the label encoding above. \n",
    "\n",
    "Try to uncomment the line below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def one_hot_encode_column(data, colname):\n",
    "    \n",
    "    categorical_feature = data[colname]\n",
    "    number_of_levels = len(np.unique(categorical_feature))\n",
    "    \n",
    "    encoder = pp.OneHotEncoder()\n",
    "    \n",
    "    one_hot_encode_feature = encoder.fit_transform(categorical_feature)\n",
    "    \n",
    "    return one_hot_encode_feature\n",
    "\n",
    "# one_hot_encode_column(data, 'MATCHUP_AWAY_TEAM')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oops! Could not convert string to float! Looks like we will have to label encode our variable first, to make it numeric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode_column(data, colname):\n",
    "    \n",
    "    data_to_encode = pd.DataFrame()\n",
    "    data_to_encode[colname] = label_encode_column(data, colname)\n",
    "    number_of_levels = len(data_to_encode[colname].unique())\n",
    "    \n",
    "    encoder = pp.OneHotEncoder(categorical_features=[0])\n",
    "    \n",
    "    one_hot_encoded_feature = encoder.fit_transform(data_to_encode).toarray()\n",
    "    one_hot_encoded_feature = pd.DataFrame(one_hot_encoded_feature)\n",
    "    \n",
    "    return one_hot_encoded_feature\n",
    "\n",
    "\n",
    "one_hot_encode_column(data, 'MATCHUP_AWAY_TEAM').sample(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Binarizing labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In alternative, we can use a label binarizer, that will return an array of binary variables, indicating whether the label is present for each observation or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binarize_labels_column(data, colname):\n",
    "    \n",
    "    categorical_feature = data[colname]\n",
    "    \n",
    "    encoder = pp.LabelBinarizer()\n",
    "    \n",
    "    return encoder.fit_transform(categorical_feature)\n",
    "\n",
    "binarize_labels_column(data, 'MATCHUP_AWAY_TEAM')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converted to a dataframe, the output is similar to that of the one hot encoder above.\n",
    "\n",
    "Such a binarizer can be useful to apply a so-called one-vs-all scheme, that consists in learning a model per class, when combined with `sklearn`'s pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(binarize_labels_column(data, 'MATCHUP_AWAY_TEAM')).sample(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please note that, unlike in `Pandas`, there is no automatic way to apply a leave-one-out approach (n-1 features), which may lead to collinearity problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Feature scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take two features in very different scales. As told in the presentation, many machine learning algorithms employ distances, which are sensitive to scale.\n",
    "\n",
    "This way, features on a bigger scale will have a disproportionate impact.\n",
    "\n",
    "Let's take two features, touch time and shot distance, and try to get the on the same scale.\n",
    "\n",
    "Please note there are input errors in the touch time column (negative touch time), and we will clean them before starting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[data['TOUCH_TIME'] > 0].copy() \n",
    "\n",
    "def scatter_plot_two_features(data, colnames, total_samples):\n",
    "\n",
    "    scatter_plot_data = data[[colnames[0], colnames[1]]].sample(n=total_samples)\n",
    "    x = scatter_plot_data[colnames[0]]\n",
    "    y = scatter_plot_data[colnames[1]]\n",
    "    \n",
    "    plot = plt.scatter(x, y)\n",
    "    \n",
    "    return plot\n",
    "\n",
    "\n",
    "cols = ['TOUCH_TIME', 'SHOT_DIST']\n",
    "scatter_plot_two_features(data, cols, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 4.1 Rescaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A possible standardization technique is scaling features between a given minimum and maximum value, often between zero and one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rescale_columns(data, colnames, total_samples_preview):\n",
    "    \n",
    "    scaler = pp.MinMaxScaler()\n",
    "    \n",
    "    rescaled_data = data[colnames]\n",
    "    rescaled_data = scaler.fit_transform(rescaled_data)\n",
    "    rescaled_data = pd.DataFrame(rescaled_data, columns=colnames)\n",
    "    \n",
    "    rescaled_data_sample = rescaled_data.sample(n=5)\n",
    "    \n",
    "    scatter_plot_two_features(rescaled_data, colnames, total_samples_preview)\n",
    "    \n",
    "    return rescaled_data_sample\n",
    "\n",
    "\n",
    "rescale_columns(data, cols, 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, the `MaxAbsScaler()` scaler (used in similar fashion) would return data in the range [-1, 1]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Standardizing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standardization is a common to make features look more or less look like standard normally distributed data: Gaussian with 0 mean and unit variance.\n",
    "\n",
    "In practice, we often ignore the shape of the distribution and just transform the data to:\n",
    "\n",
    "* Center it by removing the mean value of each feature\n",
    "* Scale it by dividing non-constant features by their standard deviation.\n",
    "\n",
    "One can assume that, if the data size is huge, both training and validation sets can be approximately viewed as normal distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_columns(data, colnames, total_samples_preview):\n",
    "    \n",
    "    scaler = pp.StandardScaler()\n",
    "    \n",
    "    rescaled_data = data[colnames]\n",
    "    rescaled_data = scaler.fit_transform(rescaled_data)\n",
    "    rescaled_data = pd.DataFrame(rescaled_data, columns=colnames)\n",
    "    \n",
    "    rescaled_data_sample = rescaled_data.sample(n=5)\n",
    "    \n",
    "    scatter_plot_two_features(rescaled_data, colnames, total_samples_preview)\n",
    "    \n",
    "    return rescaled_data_sample\n",
    "\n",
    "\n",
    "standardize_columns(data, cols, 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Normalizing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalization is the process of scaling individual samples to have unit norm, i.e. into a vector of length 1.\n",
    "\n",
    "This process can be useful if you plan to quantify the similarity of any pair of samples, using a dot-product for example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_columns(data, colnames, total_samples_preview):\n",
    "    \n",
    "    scaler = pp.Normalizer()\n",
    "    \n",
    "    rescaled_data = data[colnames]\n",
    "    rescaled_data = scaler.fit_transform(rescaled_data)\n",
    "    rescaled_data = pd.DataFrame(rescaled_data, columns=colnames)\n",
    "    \n",
    "    rescaled_data_sample = rescaled_data.sample(n=5)\n",
    "    \n",
    "    scatter_plot_two_features(rescaled_data, colnames, total_samples_preview)\n",
    "    \n",
    "    return rescaled_data_sample\n",
    "\n",
    "\n",
    "normalize_columns(data, cols, 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Binarizing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another strategy we can use is binarization.\n",
    "\n",
    "Feature binarization is the process of thresholding numerical features to get boolean values.\n",
    "\n",
    "Here, for example, we can place a threshold in shot distance at 23.9 feet to create a new binary variables: the three point line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binarize_columns(data, colnames, threshold):\n",
    "    \n",
    "    scaler = pp.Binarizer(threshold=threshold)\n",
    "\n",
    "    binarized_data = data\n",
    "    binarized_data['3_POINTER'] = scaler.fit_transform(binarized_data[colnames].values.reshape(-1, 1))\n",
    "    binarized_data = binarized_data[[colnames, '3_POINTER']]\n",
    "    binarized_data_sample = binarized_data.sample(n=5)\n",
    "    \n",
    "    return binarized_data_sample\n",
    "\n",
    "three_point_line = 23.9\n",
    "binarize_columns(data, 'SHOT_DIST', three_point_line)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
